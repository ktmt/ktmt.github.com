<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: html5 | Blog kỹ thuật máy tính]]></title>
  <link href="http://git@github.com.github.com/blog/categories/html5/atom.xml" rel="self"/>
  <link href="http://git@github.com.github.com/"/>
  <updated>2015-05-07T01:18:39+09:00</updated>
  <id>http://git@github.com.github.com/</id>
  <author>
    <name><![CDATA[kỹ thuật máy tính]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[playing with web audio api (part 2)]]></title>
    <link href="http://git@github.com.github.com/blog/2013/06/21/playing-with-web-audio-api-part-2/"/>
    <updated>2013-06-21T23:07:00+09:00</updated>
    <id>http://git@github.com.github.com/blog/2013/06/21/playing-with-web-audio-api-part-2</id>
    <content type="html"><![CDATA[<h3 id="playing-audio-with-multisource-and-precise-timing">Playing audio with multisource and precise timing</h3>
<p>Để làm game một game hay trên html5 thì âm thanh là một thứ không thể thiếu. Đầu tiên hãy đến với một game rất nổi tiếng: <a href="http://chrome.angrybirds.com/">Angry bird chrome</a></p>
<p>Các bạn có thể để ý khi play một màn game bất kì nào đấy thì sẽ thấy rất nhiều âm thanh với các lớp (layer) khác nhau, được play vào các thời điểm khác nhau một cách hợp lý. Nếu chỉ đơn thuần dùng audio tag thì việc control các layer âm thanh khác nhau được play vào thời điểm nào sẽ rất khó.</p>
<p>Nhờ có web audio api mà việc này trở nên dễ dàng hơn rất nhiều.</p>
<p>Như mình đã nói ở bài trước, web audio api xoay quanh audio context. Điều đặc biệt là <strong>một context có thể load cùng một lúc nhiều audio source</strong>, và <strong>play mỗi audio source tại các thời điểm khác nhau</strong>, với một interface rất dễ hiểu. Đầu tiên là việc load nhiều audio source vào cùng một context. Dưới đây là module <strong><em>BufferLoader</em></strong> có nhiệm vụ load các file audio từ nhiều source khác nhau và quản lý thông qua bufferList.</p>
<p>{% codeblock loadfile.js %} function BufferLoader(context, urlList, callback) { this.context = context; this.urlList = urlList; this.onload = callback; this.bufferList = new Array(); this.loadCount = 0; }</p>
<p>BufferLoader.prototype.loadBuffer = function(url, index) { // Load buffer asynchronously var request = new XMLHttpRequest(); request.open(“GET”, url, true); request.responseType = “arraybuffer”;</p>
<p>var loader = this;</p>
<p>request.onload = function() { // Asynchronously decode the audio file data in request.response loader.context.decodeAudioData( request.response, function(buffer) { if (!buffer) { alert(‘error decoding file data:’ + url); return; } loader.bufferList[index] = buffer; if (++loader.loadCount == loader.urlList.length) loader.onload(loader.bufferList); }, function(error) { console.error(‘decodeAudioData error’, error); } ); }</p>
<p>request.onerror = function() { alert(‘BufferLoader: XHR error’); }</p>
<p>request.send(); }</p>
<p>BufferLoader.prototype.load = function() { for (var i = 0; i &lt; this.urlList.length; ++i) this.loadBuffer(this.urlList[i], i); } {% endcodeblock %}</p>
<p>Giải thích đoạn code trên một chút, module BufferLoader sẽ có properties là <strong>urllist</strong> chứa một list url của các audio source, BufferList là một <strong>array các buffer</strong> tương ứng với từng source đã load.</p>
<p>Hàm <strong>load</strong> sẽ chạy từng file trong urllist, gọi hàm <strong><em>loadBuffer</em></strong> để load từng file thông qua <strong>XHR</strong>, sau khi load được qua XHR thì buffer được tạo ra và cho vào <strong>bufferList</strong> Như vậy là chúng ta đã có thể control multiple audio source thông qua <strong><em>BufferLoader</em></strong></p>
<p>Giờ đến việc play theo precise timing, như bài lần trước chúng ta đã biết để play thì chúng ta sẽ dùng hàm noteOn(t), để play từ đầu thì t = 0, thế nên để play tại một thời điểm bất kì thì chúng ta sẽ chỉ cần set t(với đơn vị là second và relative theo thời điêm bắt đầu của audio).</p>
<p>Hãy thử bằng một ví dụ đơn giản :D, bạn muốn làm 1 game bắn súng, trong đấy có một khẩu súng rất cool, giả sử là m4a1 đi :D. Chắc hẳn bạn nào đã chơi counter strike rồi thì sẽ biết là m4a1 thì phải bắt phát một được, hoặc bắn 3 viên một, hoặc bắn liên thanh. Cơ mà bạn chỉ có 1 file âm thanh chứa ngắn chứa một tiếng súng, vậy bạn phải làm sao? Rất đơn giản, bạn chỉ cần timing để playback lại cái source của bạn là ok . Hãy tham khảo đoạn code dưới đây:</p>
<p>{% codeblock loadfile.js %} MachineGun.prototype.shootRound = function (type, rounds, interval, random, random2) { if (typeof random == ‘undefined’) { random = 0; } var time = context.currentTime; for (var i = 0; i &lt; rounds; i++) { var source = this.makeSource(this.buffers[type]); source.playbackRate.value = 1 + Math.random() * random2; source.noteOn(time + i * interval + Math.random() * random); } }</p>
<p>MachineGun.prototype.makeSource = function (buffer) { var source = context.createBufferSource(); var compressor = context.createDynamicsCompressor(); var gain = context.createGainNode(); gain.gain.value = 0.2; source.buffer = buffer; source.connect(gain); gain.connect(compressor); compressor.connect(context.destination); return source; }; {% endcodeblock %}</p>
<p>Đoạn code trên có 2 hàm là <strong>shootRound</strong> và <strong>makeSource</strong>. Hàm <strong>makeSource</strong> nhận đầu vào là buffer , chỉnh lại âm thanh cho nhỏ đi một chút thông qua việc set gain = 0.2, sử dụng compressor Node để smoothing data đi một chút trước khi kết nối với destination node. Hàm <strong>shootRound</strong> có mục đích đúng như cái tên của nó, dùng để bắn, hay chính xác là timing buffer có sẵn theo các paramter đầu vào. Các parameter ở đây gồm có rounds ( là số lần lặp lại, ví dụ súng của bạn bắn 3 phát một thì rounds sẽ là 3), interval (khoảng cách giữa 2 lần bắn), 2 biến random và random2 dùng để set xem nên bắn đều đều hay bắn một cách random cho nó thật :D.</p>
<p>Chỉ nói lý thuyết không hơi khó hiểu, các bạn có thể xem ví dụ trực quan ở: <a href="https://github.com/huydx/html5collection.git" class="uri">https://github.com/huydx/html5collection.git</a> Ví dụ ở trên mình đặt ở html5collection / webaudioapi / guneffect.html. Lưu ý một chút là mình dùng XHR để load file nên các bạn sẽ phải sử dụng thông qua web server (đơn giản nhất là XAMPP) và xem thông qua localhost.</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[playing with web audio api]]></title>
    <link href="http://git@github.com.github.com/blog/2013/06/08/playing-with-web-audio-api/"/>
    <updated>2013-06-08T23:46:00+09:00</updated>
    <id>http://git@github.com.github.com/blog/2013/06/08/playing-with-web-audio-api</id>
    <content type="html"><![CDATA[<h3 id="web-audio-api-là-gì">Web audio api là gì?</h3>
<p>Web audio api là là javascript API dành cho xử lý (processing) và tái tạo(synthesizing) âm thanh. Mới được ra đời vào thời gian gần đây (first draft vào 2011/05/12), mặc dù chưa được sử dụng nhiều ( do phần nhiều là chưa được support rộng rãi, tình trạng support các bạn có thể xem ở <a href="http://caniuse.com/audio-api">đây</a> ), nhưng cá nhân mình thấy web audio api có tiềm năng ứng dụng không nhỏ, nằm ở các lý do dưới đây:</p>
<ul>
<li>Web audio cung cấp api ở tầng cao, giúp cho việc xử lý âm thanh trở nên dễ dàng hơn rất nhiều, đặc biệt bạn không phải quan tâm nhiều đến những công đoạn như xử lý binary data, phân tích FFT, encode/decode, input/output.</li>
<li>Engine để xử lý âm thanh của web audio api chạy trên một thread riêng biệt, chính vì thế một số xử lý nặng như convolve, filter sẽ không làm ảnh hưởng đến GUI thread, chính vì vậy bạn sẽ đỡ phải quan tâm nhiều đến việc phải xử lý thế nào cho nhẹ, hay cho không block UI.</li>
<li>Việc xử lý âm thanh (đặc biệt là play vào thời điểm nào- timing control) là rất quan trọng với việc làm game. Xu hướng làm rich game trên browser ngày càng phát triển, khiến cho việc sử dụng và phát triên web audio api là không thể tránh khỏi.</li>
</ul>
<p>Để tìm hiểu và sử dụng web audio api thì bài viết này mình sẽ chia làm 2 ý, đầu tiên là kiến thức cơ bản về xử lý âm thanh, và thứ hai là web audio cung cấp những gì, và sử dụng nó ra sao.</p>
<h3 id="cơ-bản-về-xử-lý-âm-thanh">Cơ bản về xử lý âm thanh</h3>
<p>Bài viết này sẽ không đi quá sâu về xử lý âm thanh, mình sẽ đi qua một số khái niệm cơ bản để giúp cho việc sử dụng web audio api dễ dàng hơn. Đầu tiên các bạn nên tham khảo trước ở một số tài liệu cơ bản về digital audio processing. Mình có google thì nó ra một cái tài liệu khá dễ hiểu ở <a href="http://smc.dei.unipd.it/education/algo4smc_ch1.pdf">đây</a>. Sau khi đọc tài liệu trên thì mình mong muốn các bạn nắm rõ được các khái niệm</p>
<ol type="1">
<li><strong>Khái niệm âm thanh là gì và được biểu diễn thế nào trên máy tính?</strong> Âm thanh là các dao động cơ học được truyền trong không khí, va đập vào màng nhĩ , làm rung màng nhĩ và kích thích não người. Trên máy tính ở dạng raw thì âm thanh thường được biểu diễn dưới dạng đồ thị dạng time-series data, tức là có 1 trục là thời gian , và một trục là độ lớn (amplitude)</li>
</ol>
<p>{% img /images/webaudioapi/sound1.png %}</p>
<p>Để biểu diễn được trên máy tính thì sẽ cần làm 2 việc, 1 là sampling (tức là một khoảng thời gian bao nhiêu lại lấy dữ liệu một lần) và 2 là quantitize , tức là với sóng có biên độ là X thì bạn phải biểu diễn X dưới dạng bit để máy tính có thể hiểu được. Như hình vẽ dưới đây thì data được sampling với thời gian là 0.1s và được sử dụng 8bit để quantitize.</p>
<p>{% img /images/webaudioapi/sound2.png %}</p>
<ol start="2" type="1">
<li><strong>Khái niệm về tần số (frequency) và biến đổi Fourier(Fourier transform)</strong>. Khái niệm về tần số các bạn có thể tìm hiểu thông qua google. Về Fourier transform, hay viết tắt là FT, thì FT giúp biên dữ liệu time series data nói chung ( ở đây là sound wave ) từ dạng ban đầu là trục độ lớn(x)/trục thời gian(y) trở thành trục độ lớn(x)/trục tần số(y). Việc biến đổi này có ích ở việc là giúp phân tích thành phần của sound wave, giúp cho ta biết trong data đó có những sóng có tần số thế nào, độ lớn ra sao. Điều này sẽ giúp cho việc nhận diện/ biểu diễn sound wave được thực hiện một cách dễ dàng hơn rất nhiều.</li>
</ol>
<p>{% img /images/webaudioapi/ft.gif %}</p>
<p>Nắm được các khái niệm trên một cách rõ ràng, chúng ta đã có thể sử dụng web audio api để làm một số việc từ đơn giản đến tương đối phức tạp rồi :).</p>
<h3 id="cấu-trúc-cơ-bản-của-web-audio-api">Cấu trúc cơ bản của web audio api</h3>
<p>Web audio api có thiết kế dưới dạng <strong>graph</strong>, được cấu thành bởi các <strong>node</strong> ( kiến trúc này còn được gọi là <strong>modular routing</strong> ). Mỗi node có thể có nhiều inputs và/hoặc nhiều outputs.</p>
<p>{% img /images/webaudioapi/graph.png %}</p>
<p>Tại sao lại là dưới dạng graph? Vì audio processing thường qua nhiều công đoạn, mỗi công đoạn xử lý lại có thể có nhiều output ( giống như hình trên ), do đó mà việc biểu diễn dưới dạng graph và connection giữa các node giúp cho việc tách các xử lý ra/ tổng hợp kết quả xử lý lại được thực hiện một cách dễ dàng và clean hơn.</p>
<h3 id="load-audio-file-một-cách-đơn-giản">Load audio file một cách đơn giản</h3>
<p>Chúng ta sẽ bắt đầu sử dụng web audio api bằng cách load và play một file audio đơn giản:</p>
<p>{% codeblock loadfile.js %} context = new webkitAudioContext(); sourceNode = context.createBufferSource(); sourceNode.connect(context.destination);</p>
<p>function loadSound(source) { var req = new XMLHttpRequest(); req.open(‘GET’, source, true); req.responseType = ‘arraybuffer’;</p>
<pre><code>req.onload = function() {
  context.decodeAudioData(req.response, function(buffer){
    playSound(buffer);
  }, function(){});
}
req.send();</code></pre>
<p>}</p>
<p>function playSound( buffer ) { sourceNode.buffer = buffer; sourceNode.noteOn(0); } {% endcodeblock %}</p>
<p>Các bạn sẽ thấy để thao tác với web audio api thì đầu tiên các bạn sẽ phải tạo ra một object gọi là AudioContext ( trên chrome sẽ gọi là WebkitAudioContext ). Toàn bộ các thao tác như tạo node mới sẽ quay quanh object này</p>
<p>Sau khi đã có context rồi thì các bước tiếp theo sẽ là :</p>
<ol type="1">
<li>Tạo ra sourceNode (sourceNode còn là Node dùng để input data)</li>
<li>Tạo ra destinationNode (là Node dùng để output data)</li>
<li>Load data thông qua XHR</li>
<li>play thông qua hàm noteOn của sourceNode (noteOn(0) tức là play tại thời điểm 0 là thời điểm bắt đầu của audio data)</li>
</ol>
<p>Các bạn có thể thấy ở trên một graph đơn giản nhất đã được tạo ra với 2 node là Source và Destination được connect với nhau.</p>
<h3 id="kết-luận">Kết luận</h3>
<p>Như vậy các bạn đã có hình dung cơ bản về web audio api.</p>
<p>Ở bài viết tiếp theo mình sẽ nói về các node xử lý cơ bản như AnalyzerNode, ConvolverNode, OscillatorNode và các ứng dụng của chúng.</p>
<p>Các bạn có thể xem bài trình bày của mình tại osaka htmlday (bài viết bằng tiếng Nhật :D):</p>
<script async class="speakerdeck-embed" data-id="8575d620b2e30130eaf25236bfdba4c0" data-ratio="1.33333333333333" src="http://git@github.com.github.com//speakerdeck.com/assets/embed.js"></script>]]></content>
  </entry>
  
</feed>
